{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic kind of model: Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic kind of layer: fully connected layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced compiling\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model: Method #1:\n",
    "# Note: only first layer needs \"input_shape\"\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model: Method #2:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling:\n",
    "# compile always takes three arguments:\n",
    "# 1. optimizer\n",
    "# 2. loss\n",
    "# 3. metrics\n",
    "\n",
    "# e.g. for multiclass classification:\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras expects Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 439us/step - loss: 0.6987 - acc: 0.5300\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.6917 - acc: 0.5170\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 0.6857 - acc: 0.5500\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.6825 - acc: 0.5650\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.6776 - acc: 0.5790\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.6742 - acc: 0.5800\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.6722 - acc: 0.5930\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.6721 - acc: 0.5690\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.6641 - acc: 0.5980\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.6621 - acc: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115847c88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classification example:\n",
    "\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1)) # Labels are either 0 or 1\n",
    "\n",
    "binary_model = Sequential()\n",
    "binary_model.add(Dense(32, activation='relu', input_dim=100))\n",
    "binary_model.add(Dense(1, activation='sigmoid'))\n",
    "binary_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "binary_model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45106348]\n",
      " [0.3374913 ]\n",
      " [0.43661577]]\n",
      "3/3 [==============================] - 0s 322us/step\n",
      "[[0.45106348]\n",
      " [0.3374913 ]\n",
      " [0.43661577]]\n",
      "(3, 1)\n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "num_rows = 3\n",
    "print(binary_model.predict(data[:num_rows, :])) # predictions for first n rows\n",
    "print(binary_model.predict(data[:num_rows, :], verbose=1)) # verbose = 1: shows timing\n",
    "print(binary_model.predict(data[:num_rows, :]).shape) # n rows, 1 column, since loss is _binary_ CE\n",
    "print(labels[:num_rows, :]) # first 10 labels\n",
    "print(binary_model.predict_classes(data[:num_rows, :])) # > or less than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [5]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "categorical_model = Sequential()\n",
    "categorical_model.add(Dense(32, activation='relu', input_dim=100))\n",
    "categorical_model.add(Dense(10, activation='softmax'))\n",
    "categorical_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "print(labels[:3, :]) # first 3 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "print(one_hot_labels[:3, :]) # one hot - it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 485us/step - loss: 2.3574 - acc: 0.0990\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 2.3111 - acc: 0.1170\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 66us/step - loss: 2.2957 - acc: 0.1220\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 2.2855 - acc: 0.1320\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 2.2748 - acc: 0.1430\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 2.2650 - acc: 0.1480\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 2.2574 - acc: 0.1550\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 2.2474 - acc: 0.1670\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 2.2373 - acc: 0.1720\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 2.2275 - acc: 0.1710\n"
     ]
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "categorical_model.fit(data, one_hot_labels, epochs=10, batch_size=32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16144872 0.07992745 0.07732411 0.12636654 0.09389471 0.10945417\n",
      "  0.07913879 0.09076368 0.10343998 0.07824185]\n",
      " [0.11248828 0.06741264 0.16594337 0.04644856 0.15298468 0.05377607\n",
      "  0.09432892 0.0687197  0.17357887 0.06431896]\n",
      " [0.08553436 0.06892415 0.1275761  0.09413425 0.08785402 0.09666151\n",
      "  0.08375759 0.11947548 0.14359014 0.09249242]]\n",
      "3/3 [==============================] - 0s 334us/step\n",
      "[[0.16144872 0.07992745 0.07732411 0.12636654 0.09389471 0.10945417\n",
      "  0.07913879 0.09076368 0.10343998 0.07824185]\n",
      " [0.11248828 0.06741264 0.16594337 0.04644856 0.15298468 0.05377607\n",
      "  0.09432892 0.0687197  0.17357887 0.06431896]\n",
      " [0.08553436 0.06892415 0.1275761  0.09413425 0.08785402 0.09666151\n",
      "  0.08375759 0.11947548 0.14359014 0.09249242]]\n",
      "(3, 10)\n",
      "[[0]\n",
      " [5]\n",
      " [5]]\n",
      "[0 8 8]\n"
     ]
    }
   ],
   "source": [
    "num_rows = 3\n",
    "print(categorical_model.predict(data[:num_rows, :])) # predictions for first n rows\n",
    "print(categorical_model.predict(data[:num_rows, :], verbose=1)) # verbose = 1: shows timing\n",
    "print(categorical_model.predict(data[:num_rows, :]).shape) # n rows, 1 column, since loss is _binary_ CE\n",
    "print(labels[:num_rows, :]) # first 10 labels\n",
    "print(categorical_model.predict_classes(data[:num_rows, :])) # > or less than 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "# 100 \"data points\", 100 x 100 images, 3 channels\n",
    "x_train = np.random.random((100, 100, 100, 3)) \n",
    "\n",
    "# one hot encoded 10 class labels, as above\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)  \n",
    "\n",
    "# 20 \"data points\", 100 x 100 images, 3 channels\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "\n",
    "# one hot encoded 10 class labels, as above\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 32]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.layers[0]\n",
    "w0 = a.weights[0]\n",
    "w0.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1\n",
      "Name conv2d_1\n",
      "Input shape (None, 100, 100, 3)\n",
      "Output shape (None, 98, 98, 32)\n",
      "[3, 3, 3, 32]\n",
      "[32]\n",
      "Layer 2\n",
      "Name conv2d_2\n",
      "Input shape (None, 98, 98, 32)\n",
      "Output shape (None, 96, 96, 32)\n",
      "[3, 3, 32, 32]\n",
      "[32]\n",
      "Layer 3\n",
      "Name max_pooling2d_1\n",
      "Input shape (None, 96, 96, 32)\n",
      "Output shape (None, 48, 48, 32)\n",
      "No weights for this layer\n",
      "Layer 4\n",
      "Name dropout_1\n",
      "Input shape (None, 48, 48, 32)\n",
      "Output shape (None, 48, 48, 32)\n",
      "No weights for this layer\n",
      "Layer 5\n",
      "Name conv2d_3\n",
      "Input shape (None, 48, 48, 32)\n",
      "Output shape (None, 46, 46, 64)\n",
      "[3, 3, 32, 64]\n",
      "[64]\n",
      "Layer 6\n",
      "Name conv2d_4\n",
      "Input shape (None, 46, 46, 64)\n",
      "Output shape (None, 44, 44, 64)\n",
      "[3, 3, 64, 64]\n",
      "[64]\n",
      "Layer 7\n",
      "Name max_pooling2d_2\n",
      "Input shape (None, 44, 44, 64)\n",
      "Output shape (None, 22, 22, 64)\n",
      "No weights for this layer\n",
      "Layer 8\n",
      "Name dropout_2\n",
      "Input shape (None, 22, 22, 64)\n",
      "Output shape (None, 22, 22, 64)\n",
      "No weights for this layer\n",
      "Layer 9\n",
      "Name flatten_1\n",
      "Input shape (None, 22, 22, 64)\n",
      "Output shape (None, 30976)\n",
      "No weights for this layer\n",
      "Layer 10\n",
      "Name dense_34\n",
      "Input shape (None, 30976)\n",
      "Output shape (None, 256)\n",
      "[30976, 256]\n",
      "[256]\n",
      "Layer 11\n",
      "Name dropout_3\n",
      "Input shape (None, 256)\n",
      "Output shape (None, 256)\n",
      "No weights for this layer\n",
      "Layer 12\n",
      "Name dense_35\n",
      "Input shape (None, 256)\n",
      "Output shape (None, 10)\n",
      "[256, 10]\n",
      "[10]\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.layers):\n",
    "    print(\"Layer\", i+1)\n",
    "    print(\"Name\", layer.name)\n",
    "    print(\"Input shape\", layer.input_shape)\n",
    "    print(\"Output shape\", layer.output_shape)\n",
    "    if not layer.weights:\n",
    "        print(\"No weights for this layer\")\n",
    "    for weight in layer.weights:\n",
    "        print(weight.shape.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 2.3410\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 6s 60ms/step - loss: 2.3050\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 2.3193\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 5s 55ms/step - loss: 2.2885\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 2.2776\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 2.2834\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 2.2705\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 2.2839\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 5s 54ms/step - loss: 2.2871\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 5s 53ms/step - loss: 2.2753\n",
      "20/20 [==============================] - 1s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
